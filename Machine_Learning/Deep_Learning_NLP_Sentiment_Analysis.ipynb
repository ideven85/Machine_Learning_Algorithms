{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feed Forward Neural Network using NLTK Sentiment Analyzer\n",
    "Loading and Cleaning Reviews\n",
    "The text data is already pretty clean, so not much preparation is required.\n",
    "Without getting too much into the details, we will prepare the data using the following method:\n",
    "* Split tokens on white space.\n",
    "* Remove all punctuation from words.\n",
    "* Remove all words that are not purely comprised of alphabetical characters.\n",
    "* Remove all words that are known stop words.\n",
    "* Remove all words that have a length <= 1 character.\n"
   ],
   "id": "ac5514c47c385705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T03:10:50.728547Z",
     "start_time": "2024-08-29T03:10:47.722428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.legacy.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ],
   "id": "159a459de312efb7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T05:56:11.395244Z",
     "start_time": "2024-08-29T05:56:11.388601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(stopwords.words(\"english\")))\n",
    "l = list(stopwords.words(\"english\"))\n",
    "#print(' '.join(l))\n",
    "print(string.punctuation)"
   ],
   "id": "1957eda0edbed6c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T03:42:15.731261Z",
     "start_time": "2024-08-20T03:42:15.727199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(stopwords.words(\"Spanish\")[:5])\n",
    "print(stopwords.words(\"Hinglish\")[:5])"
   ],
   "id": "42191f5d15846bb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en']\n",
      "['a', 'aadi', 'aaj', 'aap', 'aapne']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:00:19.197741Z",
     "start_time": "2024-08-20T04:00:19.193803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(set(stopwords.words(\"english\")))"
   ],
   "id": "5f4bda76ab8e9e2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:08:54.164132Z",
     "start_time": "2024-08-20T04:08:54.159740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_from_file(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def clean_document(doc):\n",
    "    tokens = doc.split()\n",
    "\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if not w in stop_words and len(w) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_tokens(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        tokens = f.read()\n",
    "    tokens = tokens.split()\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = set(\n",
    "        [\n",
    "            w.translate(table)\n",
    "            for w in tokens\n",
    "            if (w.isalpha() and len(w) > 1 and w not in stop_words)\n",
    "        ]\n",
    "    )\n",
    "    return tokens"
   ],
   "id": "f995a76dbbee7f11",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:11:22.058347Z",
     "start_time": "2024-08-20T04:11:22.052752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = load_data_from_file(\"data/review_polarity/txt_sentoken/pos/cv000_29590.txt\")\n",
    "tokens = set(clean_document(data))\n",
    "print(len(set(tokens)))\n",
    "z = get_tokens(\"data/review_polarity/txt_sentoken/pos/cv000_29590.txt\")"
   ],
   "id": "e5c5ace5547b1c6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:15:22.790547Z",
     "start_time": "2024-08-20T04:15:22.787782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_difference(set1, set2):\n",
    "    diff = []\n",
    "    for w in set1:\n",
    "        if w not in set2:\n",
    "            diff.append(w)\n",
    "    return diff\n",
    "\n",
    "\n",
    "len(z - tokens)\n",
    "print(get_difference(z, tokens))"
   ],
   "id": "f606f7064e9339cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:14.269281Z",
     "start_time": "2024-08-20T04:17:14.265928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def update_vocab(directory, vocab, skip=None):\n",
    "    for fileName in os.listdir(directory):\n",
    "        fileName = directory + \"/\" + fileName\n",
    "        if skip and fileName.startswith(skip):\n",
    "            continue\n",
    "\n",
    "        tokens = get_tokens(fileName)\n",
    "        vocab.update(tokens)"
   ],
   "id": "1ba5f7475075bcb8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:17.009663Z",
     "start_time": "2024-08-20T04:17:17.006681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "positive = \"data/review_polarity/txt_sentoken/pos\"\n",
    "negative = \"data/review_polarity/txt_sentoken/neg\"\n",
    "vocab = Counter()"
   ],
   "id": "c8748d493af74869",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:20.622681Z",
     "start_time": "2024-08-20T04:17:20.175167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "update_vocab(positive, vocab, \"cv9\")"
   ],
   "id": "3bd677b927bd114e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:21.114547Z",
     "start_time": "2024-08-20T04:17:21.110188Z"
    }
   },
   "cell_type": "code",
   "source": "vocab.most_common(50)",
   "id": "446d7a29ab89f008",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 883),\n",
       " ('one', 881),\n",
       " ('movie', 727),\n",
       " ('like', 710),\n",
       " ('time', 624),\n",
       " ('even', 619),\n",
       " ('also', 604),\n",
       " ('good', 577),\n",
       " ('much', 569),\n",
       " ('story', 558),\n",
       " ('would', 535),\n",
       " ('first', 535),\n",
       " ('well', 531),\n",
       " ('get', 525),\n",
       " ('way', 525),\n",
       " ('see', 521),\n",
       " ('character', 519),\n",
       " ('two', 515),\n",
       " ('make', 495),\n",
       " ('best', 480),\n",
       " ('characters', 479),\n",
       " ('life', 470),\n",
       " ('little', 456),\n",
       " ('many', 455),\n",
       " ('people', 448),\n",
       " ('films', 446),\n",
       " ('never', 432),\n",
       " ('really', 431),\n",
       " ('could', 411),\n",
       " ('man', 411),\n",
       " ('new', 410),\n",
       " ('great', 408),\n",
       " ('scene', 398),\n",
       " ('makes', 396),\n",
       " ('scenes', 390),\n",
       " ('another', 389),\n",
       " ('still', 382),\n",
       " ('back', 373),\n",
       " ('director', 369),\n",
       " ('go', 368),\n",
       " ('plot', 367),\n",
       " ('work', 367),\n",
       " ('movies', 367),\n",
       " ('end', 366),\n",
       " ('something', 365),\n",
       " ('made', 359),\n",
       " ('know', 356),\n",
       " ('however', 355),\n",
       " ('take', 351),\n",
       " ('seen', 351)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:22.903939Z",
     "start_time": "2024-08-20T04:17:22.490355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "update_vocab(negative, vocab, \"cv9\")"
   ],
   "id": "db51c1d141b0d058",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:17:25.504179Z",
     "start_time": "2024-08-20T04:17:25.497956Z"
    }
   },
   "cell_type": "code",
   "source": "vocab.most_common(50)",
   "id": "dc0942fc4b721fee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 1740),\n",
       " ('film', 1723),\n",
       " ('movie', 1524),\n",
       " ('like', 1468),\n",
       " ('even', 1289),\n",
       " ('time', 1222),\n",
       " ('good', 1149),\n",
       " ('much', 1128),\n",
       " ('would', 1112),\n",
       " ('get', 1083),\n",
       " ('also', 1070),\n",
       " ('story', 1059),\n",
       " ('two', 1024),\n",
       " ('first', 1019),\n",
       " ('character', 1012),\n",
       " ('way', 1008),\n",
       " ('make', 1003),\n",
       " ('well', 987),\n",
       " ('see', 967),\n",
       " ('characters', 954),\n",
       " ('little', 914),\n",
       " ('plot', 875),\n",
       " ('could', 872),\n",
       " ('really', 867),\n",
       " ('never', 847),\n",
       " ('people', 842),\n",
       " ('best', 835),\n",
       " ('films', 823),\n",
       " ('director', 798),\n",
       " ('another', 789),\n",
       " ('many', 788),\n",
       " ('scene', 777),\n",
       " ('life', 777),\n",
       " ('scenes', 768),\n",
       " ('bad', 760),\n",
       " ('man', 760),\n",
       " ('new', 758),\n",
       " ('know', 748),\n",
       " ('end', 737),\n",
       " ('go', 734),\n",
       " ('made', 728),\n",
       " ('movies', 726),\n",
       " ('makes', 720),\n",
       " ('back', 717),\n",
       " ('something', 716),\n",
       " ('work', 712),\n",
       " ('still', 700),\n",
       " ('great', 694),\n",
       " ('seems', 686),\n",
       " ('better', 678)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:18:07.862082Z",
     "start_time": "2024-08-20T04:18:07.859405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(vocab)  # 46557\n",
    "print(len(vocab))  # 46557\n",
    "# 37589"
   ],
   "id": "8c95a4d67e7fec6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37589\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:18:55.624054Z",
     "start_time": "2024-08-20T04:18:55.620944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_vocab(file, vocabulary, seperator):\n",
    "    with open(file, \"w\") as f:\n",
    "        contents = seperator.join([k for k, v in vocabulary.items() if v >= 2])\n",
    "        f.write(contents)"
   ],
   "id": "d9c02d59b92a70ca",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:19:25.199070Z",
     "start_time": "2024-08-20T04:19:25.193592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_vocab(\"vocab1.txt\", vocab, \"\\n\")"
   ],
   "id": "d44954b96589a8fc",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:31:12.236619Z",
     "start_time": "2024-08-20T04:31:12.233454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def doc_to_line(fileName, vocab):\n",
    "    \"\"\"\n",
    "    This will load a document, clean it, filter out tokens not in the vocabulary, then return the document as a string of white space separated tokens.\n",
    "    \"\"\"\n",
    "    doc = load_data_from_file(fileName)\n",
    "    tokens = clean_document(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return \" \".join(tokens)"
   ],
   "id": "caa89822b2190c8e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:37:13.052525Z",
     "start_time": "2024-08-20T04:37:13.049155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_docs(directory, vocab, skip):\n",
    "    lines = []\n",
    "    for fileName in os.listdir(directory):\n",
    "        fileName = directory + \"/\" + fileName\n",
    "        if skip and fileName.startswith(skip):\n",
    "            continue\n",
    "        line = doc_to_line(fileName, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ],
   "id": "f4260d83cb299c77",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:37:47.727263Z",
     "start_time": "2024-08-20T04:37:47.717856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_filename = \"vocab.txt\"\n",
    "vocab = load_data_from_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ],
   "id": "1974d9ff8cf1aac8",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:39:05.607570Z",
     "start_time": "2024-08-20T04:39:04.628213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "positive_lines = process_docs(positive, vocab, \"cv9\")\n",
    "negative_lines = process_docs(negative, vocab, \"cv9\")"
   ],
   "id": "e026d06b28dfa7d",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:39:17.129100Z",
     "start_time": "2024-08-20T04:39:17.126714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(positive_lines), len(negative_lines))"
   ],
   "id": "827422a2acbeb9f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Movie Reviews to Bag-of-Words Vectors\n",
    "\n",
    "**We will use the Keras API to convert reviews to encoded document vectors**\n",
    "\n",
    "Keras provides the Tokenize class that can do some of the cleaning and vocab definition tasks that we took care of in the previous section.\n",
    "\n",
    "It is better to do this ourselves to know exactly what was done and why. Nevertheless, the Tokenizer class is convenient and will easily transform documents into encoded vectors.First, the Tokenizer must be created, then fit on the text documents in the training dataset.In this case, these are the aggregation of the positive_lines and negative_lines arrays developed in the previous section."
   ],
   "id": "ae17052f9204c658"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:52:41.914796Z",
     "start_time": "2024-08-20T04:52:41.912079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(positive_lines[0])"
   ],
   "id": "7a30ac370702fe45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assume nothing phrase perhaps one used first impressions rumors hardly ever seem phrase especially goes oscar novak architect main focus three tango delightful funny romantic comedy assumptions novak matthew perry shy clumsy chicago based architect along openly gay partner peter steinberg oliver platt fights projects day day one job restoring popular building charles newman dylan mcdermott rich wellknown businessman charles immediately takes liking oscar enjoys personality sense humor seeing oscar someone could trust charles asks watch girlfriend unpredictable adventurous girl named amy post neve campbell makes living blowing glass charles wants know talks goes point make sure shes seeing someone else course oscar gladly takes job meets amy art show sparks fly two get go oscar feels found one meant content idea amy well another popular phrase good things must come end stays true oscar well charles walks amy oscar drink one night oscar amy become great friends doesnt seem mind thinks oscar gay hes afraid share either oscar stands shock words swear werent gay oscar id kill muttered flamboyantly charles mouth word spreads instantly town oscar come supposed gayness tell everyone isnt one would immediately think would deny fact numerous occurrences come oscar result fact denies fact could lose job charles matthew perry doesnt escape character chandler already classic comedy friends oscar novak chandler clueless shy sensitive nonetheless perry hilarious shows handle drama obviously character suffers quite bit wonderful see neve campbell outside horror movie star scream upcoming scream handles comedy superbly voice smile personality perfect romantic comedies stay genre neve neve delightful conflicted character feels love oscar knows based rumors gay usual campbell likable likable character unlike two leads dylan mcdermott flat dialogue never convincing character present mcdermott sets dull tone scene horrible acting stick drama practice dylan major weak spot three tango direction damon originality technique used whatsoever three tango lucky script edgy perry campbell wonderful comedy else film would disaster plain boring look three tango film done many times plot suspiciously close object affection plot never completed well three tangos script written rodney patrick vaccaro mckenna fun fast funny delivering original hilarious gay jokes run mill material certain dialogue characters always keeps smiling unlike last summers south park bigger longer uncut three tango comedy gay element crude vulgar script wise take route gays cant dont think anyway offended light playful comedy prove gay couple audience laughing constantly three tangos climax hilarious clever scene pure irony based outcome identity comedies three tango gem bottom line three tango light sharp snappy romantic comedy superb ending great stars one better romantic comedies\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:53:15.599157Z",
     "start_time": "2024-08-20T04:53:15.367176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = positive_lines + negative_lines\n",
    "tokenizer.fit_on_texts(docs)"
   ],
   "id": "e19f39f02fe21394",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T04:54:19.714212Z",
     "start_time": "2024-08-20T04:54:19.407904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# encode training data set\n",
    "Xtrain = tokenizer.texts_to_matrix(docs, mode=\"freq\")\n",
    "print(Xtrain.shape)"
   ],
   "id": "b95b92cabf0e8d72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 27140)\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:02:28.355945Z",
     "start_time": "2024-08-20T05:02:28.353923Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(docs))",
   "id": "dd15c67552e96885",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:02:54.575470Z",
     "start_time": "2024-08-20T05:02:54.572393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, test = docs[:1800], docs[1800:]"
   ],
   "id": "451e7bca16207152",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:03:40.891674Z",
     "start_time": "2024-08-20T05:03:40.617864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtrain = tokenizer.texts_to_matrix(train, mode=\"freq\")"
   ],
   "id": "7b2c6fe28f0bcc84",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:03:45.347742Z",
     "start_time": "2024-08-20T05:03:45.344976Z"
    }
   },
   "cell_type": "code",
   "source": "Xtrain.shape",
   "id": "ca5cbb009d82d46f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 27140)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:03:55.891094Z",
     "start_time": "2024-08-20T05:03:55.886544Z"
    }
   },
   "cell_type": "code",
   "source": "Xtrain[0]",
   "id": "639b0c028cec1773",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00472813, 0.0141844 , ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:04:20.388086Z",
     "start_time": "2024-08-20T05:04:20.345516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtest = tokenizer.texts_to_matrix(test, mode=\"freq\")"
   ],
   "id": "e772fedc57bc7ab5",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:04:34.050667Z",
     "start_time": "2024-08-20T05:04:34.048008Z"
    }
   },
   "cell_type": "code",
   "source": "Xtest.shape",
   "id": "298a5c8ed4ef3225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 27140)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:17:07.843413Z",
     "start_time": "2024-08-20T05:17:07.838737Z"
    }
   },
   "cell_type": "code",
   "source": "# todo Import Sequential from tensorflow.keras and compile the models",
   "id": "176e62ec8371b8ab",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:33:23.136904Z",
     "start_time": "2024-08-20T05:33:22.841863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ],
   "id": "2bec96a12b55fc75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:29:25.598345Z",
     "start_time": "2024-08-20T05:29:25.597129Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cf75a1980c603cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Spacy is not intended for learning but to deploy deep learning models\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning."
   ],
   "id": "68e586ba3b03a3a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:30:54.927264Z",
     "start_time": "2024-08-20T05:30:54.925185Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(list(doc.text)))",
   "id": "36dbf57f8ecba700",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T05:31:00.550431Z",
     "start_time": "2024-08-20T05:31:00.547038Z"
    }
   },
   "cell_type": "code",
   "source": "doc.text",
   "id": "e4d2d9214e8d8394",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sentence.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "397c0e92b13bdcd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
